{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask,jsonify,abort,make_response,request,url_for\n",
    "import pickle\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.probability import FreqDist\n",
    "from heapq import nlargest\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "# nltk.download('punkt') \n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize        #common\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx   # Converting sim_mat into a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)\n",
    "\n",
    "\n",
    "@app.route(\"/api\",methods=[\"POST\"])\n",
    "def model_predicts():\n",
    "    input_data = request.get_json(force=True)\n",
    "#     data = [[input_data[\"s1\"],input_data[\"s2\"],input_data[\"s3\"],input_data[\"s4\"]]]\n",
    "#     print(data)\n",
    "#     data = np.array(data,np.float32)\n",
    "#     y_pred = mymodel.predict(data)\n",
    "    \n",
    "#     y_response = [float(y_pred[0])]\n",
    "    y_response = mysum(input_data[\"data\"])\n",
    "    print(y_response)\n",
    "\n",
    "    return jsonify({'results':y_response})\n",
    "\n",
    "if __name__=='__main__':\n",
    "    app.run(port=9000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(sen):\n",
    "    sen_new = \" \".join([i for i in sen if i not in stop_words])\n",
    "    return sen_new\n",
    "\n",
    "\n",
    "\n",
    "def myfunction():\n",
    "    df = request.getjson(forced=True)\n",
    "    sentences = [sent_tokenize(x) for x in df['message']]\n",
    "    sentences = [y for x in sentences for y in x] # flatten list\n",
    "    print(\"{0} unread chats found with total of {1} sentences in it.\".format(df.shape[0], len(sentences)))\n",
    "    \n",
    "    # Extract word vectors #common\n",
    "    word_embeddings = {}\n",
    "    f = open('glove.6B.100d.txt', encoding='utf-8')\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        word_embeddings[word] = coefs\n",
    "    f.close()\n",
    "    \n",
    "    # remove punctuations, numbers and special characters\n",
    "    clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
    "\n",
    "    # make alphabets lowercase\n",
    "    clean_sentences = [s.lower() for s in clean_sentences]\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words = stopwords.words('english')\n",
    "    # remove stopwords from the sentences  #common\n",
    "    clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n",
    "    \n",
    "    \n",
    "    \n",
    "    sentence_vectors = []\n",
    "    for i in clean_sentences:\n",
    "      if len(i) != 0:\n",
    "        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
    "      else:\n",
    "        v = np.zeros((100,))\n",
    "      sentence_vectors.append(v)\n",
    "    \n",
    "    # similarity matrix\n",
    "    sim_mat = np.zeros([len(sentences), len(sentences)])\n",
    "    \n",
    "    \n",
    "\n",
    "    for i in range(len(sentences)):\n",
    "      for j in range(len(sentences)):\n",
    "        if i != j:\n",
    "          sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]\n",
    "        \n",
    "    nx_graph = nx.from_numpy_array(sim_mat)\n",
    "    scores = nx.pagerank(nx_graph)\n",
    "    \n",
    "    \n",
    "    class color:\n",
    "    PURPLE = '\\033[95m'\n",
    "    CYAN = '\\033[96m'\n",
    "    DARKCYAN = '\\033[36m'\n",
    "    BLUE = '\\033[94m'\n",
    "    GREEN = '\\033[92m'\n",
    "    YELLOW = '\\033[93m'\n",
    "    RED = '\\033[91m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "    END = '\\033[0m'\n",
    "    \n",
    "    ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
    "\n",
    "    # Chat Analytics\n",
    "    print(color.RED+\"{0}\".format(\"Atharv\")+color.END+\" has {0} chats\".format(6))\n",
    "    print(color.RED+\"{0}\".format(\"Krunal\")+color.END+\" has {1} chats\".format(\"Krunal\", 16))\n",
    "    print(\"\\nTotal \"+color.GREEN+\"{0}\".format(22)+color.END+\" chat messages received, with {0} participants\\n\".format(2))\n",
    "    print(color.BOLD+\"Summary:\\n\"+color.END)\n",
    "    for i in range(7):\n",
    "      print(ranked_sentences[i][1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
